{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "def convert_to_timestamp(time_string):\n",
    "    # Các định dạng thời gian có thể có\n",
    "    formats = [\n",
    "        \"%Y-%m-%dT%H:%M:%S.%fZ\",  # Ví dụ: 2024-12-12T10:03:00.781Z\n",
    "        \"%Y-%m-%dT%H:%M:%SZ\",     # Ví dụ: 2024-12-12T10:03:00Z\n",
    "        \"%Y-%m-%d %H:%M:%S\",       # Ví dụ: 2024-12-12 10:03:00\n",
    "        \"%d/%m/%Y %H:%M:%S\",       # Ví dụ: 12/12/2024 10:03:00\n",
    "        \"%m/%d/%Y %H:%M:%S\",       # Ví dụ: 12/12/2024 10:03:00\n",
    "        \"%Y-%m-%d\",                # Ví dụ: 2024-12-12\n",
    "        \"%d/%m/%Y\"                 # Ví dụ: 12/12/2024\n",
    "    ]\n",
    "    \n",
    "    # Thử qua các định dạng để phân tích chuỗi thời gian\n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            # Chuyển đổi thành datetime và sau đó sang timestamp\n",
    "            dt = datetime.strptime(time_string, fmt)\n",
    "            return int(dt.timestamp())  # Trả về timestamp\n",
    "        except ValueError:\n",
    "            continue  # Tiếp tục nếu không thành công với định dạng này\n",
    "    \n",
    "    raise ValueError(f\"Không thể phân tích chuỗi thời gian: {time_string}\")\n",
    "\n",
    "def fetch_data_from_sitemap(xml_url: str):\n",
    "    try:\n",
    "        response = requests.get(xml_url)\n",
    "        response.raise_for_status()  \n",
    "        xml_content = response.text \n",
    "        \n",
    "        soup = BeautifulSoup(xml_content, 'xml')\n",
    "\n",
    "        urls = []\n",
    "        for url_tag in soup.find_all('url'):\n",
    "            url_data = {}\n",
    "            \n",
    "            for child in url_tag.find_all(recursive=False):\n",
    "                if child.name != 'image' and child.name != \"changefreq\": \n",
    "                    url_data[child.name] = child.text.strip() if child.text else None\n",
    "\n",
    "            image_tag = url_tag.find('image:image')\n",
    "            if image_tag:\n",
    "                image_title = image_tag.find('image:title')\n",
    "                \n",
    "                if image_title:\n",
    "                    url_data['title'] = image_title.text.strip()\n",
    "\n",
    "            if url_data.get(\"lastmod\",None):\n",
    "                lastmod= convert_to_timestamp(url_data.get(\"lastmod\",None))\n",
    "                url_data[\"lastmod\"] = lastmod\n",
    "                urls.append(url_data)\n",
    "        return urls\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "\n",
    "def get_meta_data(pages: dict,num_pages=None) -> dict:\n",
    "    data = dict()\n",
    "    if pages is not None:\n",
    "        for page in pages:\n",
    "            props = page[\"properties\"]\n",
    "            # print(props)\n",
    "            page_name = props[\"page_name\"][\"title\"][0][\"text\"][\"content\"]\n",
    "            page_id = props[\"page_id\"][\"rich_text\"][0][\"text\"][\"content\"]\n",
    "            # print(page_id)\n",
    "            data[page_name] = page_id\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sitemap_products = fetch_data_from_sitemap(\"https://shop.joygarden.vn/sitemap_products_1.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "print(os.getenv(\"NOTION_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Thêm thư mục cha vào sys.path\n",
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "\n",
    "from notion_client  import NotionClient\n",
    "from crawler import WebCrawler\n",
    "client = NotionClient(notion_token=os.)\n",
    "crawler = WebCrawler(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_parts(text, max_length=2000):\n",
    "    \"\"\"\n",
    "    Splits the text into smaller chunks, each no longer than `max_length` characters.\n",
    "\n",
    "    Args:\n",
    "    - text (str): The long text to split.\n",
    "    - max_length (int): The maximum length of each chunk (default is 2000 characters).\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of text segments, each with length ≤ `max_length`.\n",
    "    \"\"\"\n",
    "    # List to store the text chunks\n",
    "    chunks = []\n",
    "    \n",
    "    # Split the text into chunks of max_length\n",
    "    for i in range(0, len(text), max_length):\n",
    "        chunks.append(text[i:i + max_length])\n",
    "    \n",
    "    return chunks\n",
    "def crawl_and_store_notion(data : dict, page_id : str):\n",
    "    try:\n",
    "        product_info = crawler.get_product_info(data['loc'])\n",
    "        chunks = split_text_into_parts(product_info)\n",
    "        new_page_reponse = client.create_new_page(page_id,data['title'])\n",
    "        if new_page_reponse.status_code != 200:\n",
    "            print(\"page_id invalid\")\n",
    "            # os._exit(1)\n",
    "        page_id = new_page_reponse.json().get(\"id\",None)\n",
    "        for chunk in chunks:\n",
    "            client.add_code_block(page_id,chunk)\n",
    "        data['page_id'] = page_id\n",
    "        return page_id,data\n",
    "    except:\n",
    "        return None,data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"meta_data.json\",\"w+\") as file:\n",
    "    file.write(json.dumps(meta_data,ensure_ascii=False,indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "import concurrent.futures\n",
    "meta_data = dict()\n",
    "\n",
    "PAGE_ID = \"15f6739fbff180279e11f50cb9854822\"\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
    "        futures = [executor.submit(crawl_and_store_notion, data,PAGE_ID) for data in sitemap_products]\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "                page_id,data = future.result()\n",
    "                if not page_id:\n",
    "                        meta_data[page_id] = data\n",
    "                else:\n",
    "                        print(\"URL error: \",data['loc'])\n",
    "                        \n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = client.get_code_blocks(\"15e6739fbff181deb2affa95dad9267e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code block 15e6739f-bff1-81ff-be96-cf346572e752 deleted successfully.\n",
      "Code block 15e6739f-bff1-816c-92b8-eb022d3bf0b7 deleted successfully.\n"
     ]
    }
   ],
   "source": [
    "client.clear_code_blocks(\"15e6739fbff181deb2affa95dad9267e\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".dagster_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
